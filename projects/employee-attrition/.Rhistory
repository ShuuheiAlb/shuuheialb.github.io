plot(a0, probLambdaBetween3And5)
p(am, bm)
library(haven)
X1ResearchProjectData <- read_sav("Downloads/1ResearchProjectData.sav")
View(X1ResearchProjectData)
install.packages(c('pbdZMQ', 'repr', 'devtools'))
install.packages(c("pbdZMQ", "repr", "devtools"))
devtools::install_github('IRkernel/IRdisplay')
devtools::install_github('IRkernel/IRkernel')
IRkernel::installspec()
install.packages('devtool')
install.packages('devtools')
install.packages(c('libfreetype6-dev','libpng-dev', 'libtiff5-dev', 'libjpeg-dev'))
install.packages(ggscatter)
install.packages("ggscatter"
)
install.packages("car")
install.packages("car", dependencies = TRUE)
install.packages("effects", dependencies = TRUE)
install.packages("car",dependencies=TRUE)
install.packages("car",repos = c("http://rstudio.org/_packages",
"http://cran.rstudio.com"))
install.packages(c("car", "pbkrtest", "lme4"),repos = c("http://rstudio.org/_packages",
"http://cran.rstudio.com"))
install.packages(c("car", "pbkrtest", "lme4"),repos = c("http://rstudio.org/_packages",
"http://cran.rstudio.com"), dependencies = TRUE)
install.packages(c("car"),repos = c("http://rstudio.org/_packages",
"http://cran.rstudio.com"), dependencies = TRUE)
install.packages(c("car"),repos = c("http://rstudio.org/_packages",
"http://cran.rstudio.com"), dependencies = TRUE)
install.packages(c("DescTools"),repos = c("http://rstudio.org/_packages",
"http://cran.rstudio.com"), dependencies = TRUE)
install.packages(c("ggfortify"),repos = c("http://rstudio.org/_packages",
"http://cran.rstudio.com"), dependencies = TRUE)
install.packages(chron)
install.packages("chron")
data <- read.csv("Hospital ER.csv")
ls
data <- read.csv("Hospital ER.csv")
pwd
strsplit(c("a b", "c d"
), split = " ")
strsplit(c("a b", "c d"), split = " ")[, 1]
install.packages("DataExplorer")
install.packages("zoo")
install.packages("sure")
install.packages("gridExtra")
install.packages(
"compound.Cox")
remove.packages(
"compound.Cox")
install.packages("survAUC")
remove.packages("survAUC")
install.packages("survminer")
remove.packages("survminer")
install.packages("splines")
install.packages("RColorBrewer")
rm(list = ls())
hr <- read.csv("hr_data.csv")
setwd("Downloads/shuuheialb.github.io/projects/employee-attrition")
setwd("Downloads/shuuheialb.github.io/projects/employee-attrition")
hr <- read.csv("hr_data.csv")
rm(list = ls())
hr <- read.csv("hr_data.csv")
install.package(ggRandomForests)
install.packages("ggRandomForests")
install.packages('rsconnect')
rsconnect::setAccountInfo(name='shuuheialb',
token='CD03F711E57CDFB596B3DCF39B99E0A1',
secret='<SECRET>')
rsconnect::setAccountInfo(name='shuuheialb',
token='CD03F711E57CDFB596B3DCF39B99E0A1',
secret='v0Fng8KpPFPLiycyx57xTChOONjKbRGNlJB41gLw')
rm(list = ls())
hr <- read.csv("hr_data.csv")
setwd("Downloads/shuuheialb.github.io/projects/employee-attrition")
rm(list = ls())
hr <- read.csv("hr_data.csv")
paste0("x", 1:5)
install.packages("tidyverse")
read.csv("hr_data.csv")
setwd("Downloads/shuuheialb.github.io/projects/employee-attrition")
read.csv("hr_data.csv")
DATA <- read.csv("hr_data.csv")
DATA["Attrition"]
str(DATA["Attrition"])
str(DATA$Attrition)
# Plotly implementation
suppressMessages(library(plotly))
# Importing data
setwd("Downloads/shuuheialb.github.io/projects/employee-attrition")
rm(list = ls())
rm(list = ls())
DATA <- read.csv("hr_data.csv")
# Save some test and validation data
set.seed(100)
SPLIT <- initial_validation_split(DATA, c(0.75, 0.15))
set.seed(100)
rm(list = ls())
DATA <- read.csv("hr_data.csv")
# Save some test and validation data
library(rsample)
set.seed(100)
SPLIT <- initial_validation_split(DATA, c(0.75, 0.15))
TEST <- testing(SPLIT)
VALID <- validation(SPLIT)
hr <- training(SPLIT)
hr_full <- rbind(hr, VALID)
# Check for incorrect/problematic entries
#head(hr, 10)
#nrow(hr[duplicated(hr), ])
#sum(is.na(hr))
#summary(hr)
#sapply(hr, function(x) {
#  unique_vals <- sort(unique(x))
#  if (length(unique_vals) <= 30) return(unique_vals)
#  return(c(unique_vals[1:30], "etc"))
#})
# Constant vectors:
# Formatted columns
cat_cols <- c("BusinessTravel", "Department", "Gender", "EducationField", "JobRole", "MaritalStatus")
bool_cols <- c("Attrition", "Over18", "OverTime")
# Irrelevant columns
unique_id_col <- "EmployeNumber"
single_value_cols <- names(hr)[sapply(hr, function (col) length(unique(col)) == 1)]
invalid_unif_cols <- c("DailyRate", "MonthlyRate", "HourlyRate")
# Added columns
new_time_cols <- c("NotWorkingYears", "YearsAtOtherCompanies")
library(survival)
library(randomForestSRC)
library(pec, warn.conflicts = FALSE)
library(DataExplorer)
library(ggplot2)
library(reshape2)
library(corrplot)
# === Models
coxph_model <- function (df) {
model <- coxph(data = df, formula = Surv(YearsAtCompany, Attrition) ~ ., method = "breslow", x = TRUE)
return(model)
}
rsf_model <- function (df) {
model <- rfsrc(Surv(YearsAtCompany, Attrition) ~ ., data = df, ntree = 100)
return(model)
}
# === Feature selection
# 1. Cox score
coxph_score <- function (df, model) { # ignore the model
features <- names(df)[!(names(df) %in% c("Attrition", "YearsAtCompany"))]
scores <- sapply(features, function (col){
summary(coxph_model(df[c("YearsAtCompany", "Attrition", col)]))$concordance[1]
})
names(scores) <- features
return(scores)
}
coxph_details <- function (model) {
return(summary(model)$coefficients[, c("exp(coef)", "coef", "se(coef)")])
}
# 2. RSF Importance
rsf_importance <- function (df, model) {
return(predict(model, df, importance = TRUE)$importance)
}
# ===
print_coxph_var_rank <- function (df) {
print("Univariate Cox score:")
res <- coxph_score(df, coxph_model(df))
# SOON: 1D line
print(res[order(res, decreasing = TRUE)])
}
print_rsf_var_rank <- function (df) {
print("Variable importance:")
res <- rsf_importance(df, rsf_model(df))
# SOON: 1D line
print(res[order(abs(res), decreasing = TRUE)])
}
# === Accuracy metrics
# 1. C-index
c_index <- function (df, model) {
c_table <- cindex(model, formula = Surv(YearsAtCompany, Attrition) ~ ., data = df)
if (unlist(c_table$Pairs) == 0) return(0) # In case of zero events
return(unlist(c_table$AppCindex))
}
# 2. PEC
# ===
print_c_index <- function(df, model_f) {
print("Concordance index:")
print(c_index(df, model_f(df)))
}
plot_pec <- function(df, model_f) { # split???
suppressMessages(pred_error <- pec(model_f(df), data = df, formula = Surv(YearsAtCompany, Attrition) ~ .,
splitMethod = "cv10", cens.model = "marginal", reference = FALSE))
plot(pred_error, xlim = c(0, 10), ylim = c(0, 0.25)) # 0.25 is the worst case scenario (random model)
title("Prediction Error Curve")
}
# === Cross-validation
cross_val <- function (df, model_f, select_f, feature_num = 1, k = 5) {
random_index <- sample(1:nrow(df))
performance_score1_vec <- numeric(k)
for (fold in 1:k) {
prev_end <- round((fold-1)/k * nrow(df))
size <- round(fold * nrow(df)/k) - round((fold-1) * nrow(df)/k)
idx <- random_index[(prev_end+1):(prev_end+size)]
test <- df[idx, ]
train <- df[-idx, ]
# General preprocessing
train <- prepare(train)
test <- prepare(test)
# Training-data preprocessing
train <- train[-outlier_index(train, "YearsAtCompany", iqr_coef = 2), ]
# Model-specific preprocessing
if (identical(model_f, coxph_model)) {
# Assumption check
cols <- colnames(train)
focus_cols <- cols[!(cols %in% c("Department", "JobRole"))]
pht <- proportional_hazard_table(model_f(train[focus_cols]))
broken_ph_assumption_cols <- names(which(pht$table[, "p"] < 0.01))
for (col in broken_ph_assumption_cols) {
inter_col <= paste0("Inter_YC_", col)
cols <- c(cols, inter_col)
train[inter_col] <- train[col] * train["YearsAtCompany"]
test[inter_col] <- test[col] * test["YearsAtCompany"]
}
} else {
# Probational data treatment
cols <- c(cols, "IsProbation")
train["IsProbation"] <- train["MonthlyIncome"] <= 2000
test["IsProbation"] <- test["MonthlyIncome"] <= 2000
}
# Initial model
model1 <- model_f(train)
feature_score <- select_f(test, model1)
feature_rank <- feature_score[order(abs(feature_score), decreasing = TRUE)]
# Correlated columns removal
feature_redundancy <- corr_table(train, limit = 0.7)
for (i in 1:nrow(feature_redundancy)) {
# Retain higher rank, if any
x <- feature_redundancy[i, "Var1"]
y <- feature_redundancy[i, "Var2"]
if (x %in% feature_rank && y %in% feature_rank) {
x_rank <- which(feature_rank == x)
y_rank <- which(feature_rank == y)
feature_rank <- feature_rank[-max(x_rank, y_rank)]
}
}
# Next model
final_features <- names(feature_rank)[1:min(feature_num, length(names(feature_rank)))]
#print("Choosing features ...")
#print(final_features)
model2 <- model_f(train[c("Attrition", "YearsAtCompany", final_features)])
performance_score1 <- c_index(test, model2)
# PEC variable soon
performance_score1_vec[fold] <- performance_score1
}
print(paste0("Performance score via concordance index, for feature number = ", feature_num, ":"))
# Box plot soon
print(paste0("Mean: ", mean(performance_score1_vec)))
print(performance_score1_vec)
print(paste0("SE: ", sd(performance_score1_vec)))
}
# ===
prepare <- function (df) {
df[cat_cols] <- lapply(df[cat_cols], factor)
df[bool_cols] <- lapply(df[bool_cols], function (col) { ifelse(col %in% c("Yes", "Y"), TRUE, FALSE) })
df["YearsAtOtherCompanies"] <- df["TotalWorkingYears"] - df["YearsAtCompany"]
df <- df[!(names(df) %in% c("EmployeeNumber", single_value_cols, invalid_unif_cols, coupled_cols, high_cor_cols))]
return(df)
}
outlier_index <- function (df, col, iqr_coef = 0) {
vec <- df[[col]]
qnt <- quantile(vec, probs = c(0.25, 0.75))
iqr <- qnt[2] - qnt[1]
min <- qnt[1] - iqr_coef * iqr
max <- qnt[2] + iqr_coef * iqr
return(which(vec < min | vec > max))
}
proportional_hazard_table <- function (model) {
return(cox.zph(model))
}
plot_correlation <- function (df, limit = -1) {
# Only numerical (unless categories can be continuously extended)
num_cols <- sapply(df, is.numeric)
bool_cols <- sapply(df, is.logical)
df2 <- df[, num_cols | bool_cols]
# Only select the high-correlated columns
cor_matrix <- cor(df2)
high_cor_pairs <- which(cor_matrix >= limit & cor_matrix < 1, arr.ind = TRUE)
high_cor_cols <- colnames(df2)[high_cor_pairs[, 1]]
ggplot(data = melt(cor_matrix[high_cor_cols, high_cor_cols]), aes(x = Var1, y = Var2, fill = value)) +
geom_tile() + scale_fill_viridis() +
xlab("") + ylab("") + ggtitle("Correlation Map") +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
plot.title = element_text(hjust = 0.5))
}
# Final iteration
rsf_cols <- c("Attrition", "YearsAtCompany", "JobLevel", "OverTime", "MonthlyIncome", "StockOptionLevel", "JobRole",
"EnvironmentSatisfaction", "WorkLifeBalance")
coxph_cols <- c("Attrition", "YearsAtCompany", "JobRole", "OverTime", "StockOptionLevel", "MaritalStatus")
rsf_final_model <- rsf_model(hr[rsf_cols])
# Final iteration
hr_full <- prepare(hr_full)
cat_cols <- c("BusinessTravel", "Department", "Gender", "EducationField", "JobRole", "MaritalStatus")
bool_cols <- c("Attrition", "Over18", "OverTime")
# Irrelevant columns
unique_id_col <- "EmployeNumber"
single_value_cols <- names(hr)[sapply(hr, function (col) length(unique(col)) == 1)]
invalid_unif_cols <- c("DailyRate", "MonthlyRate", "HourlyRate")
coupled_cols <- c("Age", "TotalWorkingYears", "YearsWithCurrManager", "YearsInCurrentRole", "YearsSinceLastPromotion")
# Added columns
new_time_cols <- c("NotWorkingYears", "YearsAtOtherCompanies")
library(survival)
library(randomForestSRC)
library(pec, warn.conflicts = FALSE)
library(DataExplorer)
library(ggplot2)
library(reshape2)
library(corrplot)
# === Models
coxph_model <- function (df) {
model <- coxph(data = df, formula = Surv(YearsAtCompany, Attrition) ~ ., method = "breslow", x = TRUE)
return(model)
}
rsf_model <- function (df) {
model <- rfsrc(Surv(YearsAtCompany, Attrition) ~ ., data = df, ntree = 100)
return(model)
}
# === Feature selection
# 1. Cox score
coxph_score <- function (df, model) { # ignore the model
features <- names(df)[!(names(df) %in% c("Attrition", "YearsAtCompany"))]
scores <- sapply(features, function (col){
summary(coxph_model(df[c("YearsAtCompany", "Attrition", col)]))$concordance[1]
})
names(scores) <- features
return(scores)
}
coxph_details <- function (model) {
return(summary(model)$coefficients[, c("exp(coef)", "coef", "se(coef)")])
}
# 2. RSF Importance
rsf_importance <- function (df, model) {
return(predict(model, df, importance = TRUE)$importance)
}
# ===
print_coxph_var_rank <- function (df) {
print("Univariate Cox score:")
res <- coxph_score(df, coxph_model(df))
# SOON: 1D line
print(res[order(res, decreasing = TRUE)])
}
print_rsf_var_rank <- function (df) {
print("Variable importance:")
res <- rsf_importance(df, rsf_model(df))
# SOON: 1D line
print(res[order(abs(res), decreasing = TRUE)])
}
# === Accuracy metrics
# 1. C-index
c_index <- function (df, model) {
c_table <- cindex(model, formula = Surv(YearsAtCompany, Attrition) ~ ., data = df)
if (unlist(c_table$Pairs) == 0) return(0) # In case of zero events
return(unlist(c_table$AppCindex))
}
# 2. PEC
# ===
print_c_index <- function(df, model_f) {
print("Concordance index:")
print(c_index(df, model_f(df)))
}
plot_pec <- function(df, model_f) { # split???
suppressMessages(pred_error <- pec(model_f(df), data = df, formula = Surv(YearsAtCompany, Attrition) ~ .,
splitMethod = "cv10", cens.model = "marginal", reference = FALSE))
plot(pred_error, xlim = c(0, 10), ylim = c(0, 0.25)) # 0.25 is the worst case scenario (random model)
title("Prediction Error Curve")
}
# === Cross-validation
cross_val <- function (df, model_f, select_f, feature_num = 1, k = 5) {
random_index <- sample(1:nrow(df))
performance_score1_vec <- numeric(k)
for (fold in 1:k) {
prev_end <- round((fold-1)/k * nrow(df))
size <- round(fold * nrow(df)/k) - round((fold-1) * nrow(df)/k)
idx <- random_index[(prev_end+1):(prev_end+size)]
test <- df[idx, ]
train <- df[-idx, ]
# General preprocessing
train <- prepare(train)
test <- prepare(test)
# Training-data preprocessing
train <- train[-outlier_index(train, "YearsAtCompany", iqr_coef = 2), ]
# Model-specific preprocessing
if (identical(model_f, coxph_model)) {
# Assumption check
cols <- colnames(train)
focus_cols <- cols[!(cols %in% c("Department", "JobRole"))]
pht <- proportional_hazard_table(model_f(train[focus_cols]))
broken_ph_assumption_cols <- names(which(pht$table[, "p"] < 0.01))
for (col in broken_ph_assumption_cols) {
inter_col <= paste0("Inter_YC_", col)
cols <- c(cols, inter_col)
train[inter_col] <- train[col] * train["YearsAtCompany"]
test[inter_col] <- test[col] * test["YearsAtCompany"]
}
} else {
# Probational data treatment
cols <- c(cols, "IsProbation")
train["IsProbation"] <- train["MonthlyIncome"] <= 2000
test["IsProbation"] <- test["MonthlyIncome"] <= 2000
}
# Initial model
model1 <- model_f(train)
feature_score <- select_f(test, model1)
feature_rank <- feature_score[order(abs(feature_score), decreasing = TRUE)]
# Correlated columns removal
feature_redundancy <- corr_table(train, limit = 0.7)
for (i in 1:nrow(feature_redundancy)) {
# Retain higher rank, if any
x <- feature_redundancy[i, "Var1"]
y <- feature_redundancy[i, "Var2"]
if (x %in% feature_rank && y %in% feature_rank) {
x_rank <- which(feature_rank == x)
y_rank <- which(feature_rank == y)
feature_rank <- feature_rank[-max(x_rank, y_rank)]
}
}
# Next model
final_features <- names(feature_rank)[1:min(feature_num, length(names(feature_rank)))]
#print("Choosing features ...")
#print(final_features)
model2 <- model_f(train[c("Attrition", "YearsAtCompany", final_features)])
performance_score1 <- c_index(test, model2)
# PEC variable soon
performance_score1_vec[fold] <- performance_score1
}
print(paste0("Performance score via concordance index, for feature number = ", feature_num, ":"))
# Box plot soon
print(paste0("Mean: ", mean(performance_score1_vec)))
print(performance_score1_vec)
print(paste0("SE: ", sd(performance_score1_vec)))
}
# ===
prepare <- function (df) {
df[cat_cols] <- lapply(df[cat_cols], factor)
df[bool_cols] <- lapply(df[bool_cols], function (col) { ifelse(col %in% c("Yes", "Y"), TRUE, FALSE) })
df["YearsAtOtherCompanies"] <- df["TotalWorkingYears"] - df["YearsAtCompany"]
df <- df[!(names(df) %in% c("EmployeeNumber", single_value_cols, invalid_unif_cols, coupled_cols, high_cor_cols))]
return(df)
}
outlier_index <- function (df, col, iqr_coef = 0) {
vec <- df[[col]]
qnt <- quantile(vec, probs = c(0.25, 0.75))
iqr <- qnt[2] - qnt[1]
min <- qnt[1] - iqr_coef * iqr
max <- qnt[2] + iqr_coef * iqr
return(which(vec < min | vec > max))
}
proportional_hazard_table <- function (model) {
return(cox.zph(model))
}
plot_correlation <- function (df, limit = -1) {
# Only numerical (unless categories can be continuously extended)
num_cols <- sapply(df, is.numeric)
bool_cols <- sapply(df, is.logical)
df2 <- df[, num_cols | bool_cols]
# Only select the high-correlated columns
cor_matrix <- cor(df2)
high_cor_pairs <- which(cor_matrix >= limit & cor_matrix < 1, arr.ind = TRUE)
high_cor_cols <- colnames(df2)[high_cor_pairs[, 1]]
ggplot(data = melt(cor_matrix[high_cor_cols, high_cor_cols]), aes(x = Var1, y = Var2, fill = value)) +
geom_tile() + scale_fill_viridis() +
xlab("") + ylab("") + ggtitle("Correlation Map") +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
plot.title = element_text(hjust = 0.5))
}
# Final iteration
hr_full <- prepare(hr_full)
high_cor_cols <- c("JobLevel")
hr_full <- prepare(hr_full)
hr <- prepare(hr)
rsf_cols <- c("Attrition", "YearsAtCompany", "JobLevel", "OverTime", "MonthlyIncome", "StockOptionLevel", "JobRole",
"EnvironmentSatisfaction", "WorkLifeBalance")
coxph_cols <- c("Attrition", "YearsAtCompany", "JobRole", "OverTime", "StockOptionLevel", "MaritalStatus")
rsf_final_model <- rsf_model(hr[rsf_cols])
install.packages("SurvMetric")
install.packages("SurvMetrics")
library(SurvMetrics)
SurvMetrics::Cindex(coxph_model(hr))
SurvMetrics::Cindex(coxph_model(hr), hr)
SurvMetrics::Cindex(coxph_model(hr[coxph_cols]), hr["YearsAtCompany"])
SurvMetrics::Cindex(coxph_model(hr[coxph_cols]), hr[coxph_cols])
class(hr["YearsAtCompany"])
str(hr["YearsAtCompany"])
is.numeric(hr["YearsAtCompany"])
IRdisplay::display_html("<iframe seamless src='plot.html' width=1000, height=600></iframe>")
rm(list = ls())
DATA <- read.csv("hr_data.csv")
# Save some test and validation data
library(rsample)
set.seed(100)
SPLIT <- initial_validation_split(DATA, c(0.75, 0.15))
TEST <- testing(SPLIT)
VALID <- validation(SPLIT)
hr <- training(SPLIT)
hr_full <- rbind(hr, VALID)
rm(list = ls())
DATA <- read.csv("hr_data.csv")
# Save some test and validation data
library(rsample)
set.seed(100)
SPLIT <- initial_validation_split(DATA, c(0.75, 0.15))
TEST <- testing(SPLIT)
VALID <- validation(SPLIT)
hr <- training(SPLIT)
hr_full <- rbind(hr, VALID)
